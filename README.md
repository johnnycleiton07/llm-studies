# Reposit√≥rio de Projetos de PLN, Machine Learning e LLM

Bem-vindo ao meu reposit√≥rio de projetos de Processamento de Linguagem Natural (PLN), Machine Learning e Modelos de Linguagem de Grande Escala (LLM). Este reposit√≥rio cont√©m uma cole√ß√£o de notebooks do Google Colab que exemplificam diversas t√©cnicas e aplica√ß√µes nessas √°reas da Intelig√™ncia Artificial.

<div align="center">
  
| ![I'm a talking robot You can trust me](files/talking_robot.gif) |
|:--:|
| *I'm a talking robot. You can trust me.* |

</div>

## Sum√°rio

1. [Introdu√ß√£o ao Processamento de Linguagem Natural](#introdu√ß√£o-ao-processamento-de-linguagem-natural)
2. [Classifica√ß√£o de Texto com Machine Learning](#classifica√ß√£o-de-texto-com-machine-learning)
3. [Redes Neurais Artificiais para PLN](#redes-neurais-artificiais-para-pln)
4. [An√°lise de Sentimentos](#an√°lise-de-sentimentos)
5. [Fine-tunning em Modelos de Linguagem de Grande Escala](#fine-tunning-em-modelos-de-linguagem-de-grande-escala)
6. [Modelos GPT da OpenAI](#modelos-gpt-da-openai)
7. [Modelo BERT e Varia√ß√µes](#modelo-bert-e-varia√ß√µes)
8. [Hugging Face e Transformers](#hugging-face-e-transformers)



## Introdu√ß√£o ao Processamento de Linguagem Natural

Este projeto aborda os conceitos b√°sicos de Processamento de Linguagem Natural (PLN), incluindo tokeniza√ß√£o, remo√ß√£o de stopwords, stemming e lematiza√ß√£o. O notebook demonstra como essas t√©cnicas podem ser aplicadas em textos simples para preparar dados textuais para tarefas de Machine Learning.

[NLP com a biblioteca NLTK](NLP_com_NLTK.ipynb)

[NLP com a biblioteca spaCy](NLP_com_spaCy.ipynb)

[Pr√©-processamento de dados de texto](Pre_processamento_com_NLTK_e_spaCy.ipynb)



## Classifica√ß√£o de Texto com Machine Learning

Neste projeto, s√£o explorados algoritmos de Machine Learning para a classifica√ß√£o de textos. Utilizando um conjunto de dados de textos de emails para treinar um modelo de classifica√ß√£o e verificar seu desempenho com as m√©tricas de avalia√ß√£o.

[Classifica√ß√£o de emails em spam e n√£o spam](Spam_email_classification_ML.ipynb)



## Redes Neurais Artificiais para PLN

Implementa√ß√£o de uma Rede Neural Artificial para Processamento de Linguagem Natural utilizando scikit-learn e Keras. E tamb√©m uma vers√£o de Rede Neural com vetores de alta dimens√£o (embeddings).

[Cria√ß√£o de rede neural artificial](Implementa√ß√£o_de_rede_neural.ipynb)

[Cria√ß√£o de rede neural artificial com embeddings](Implementa√ß√£o_de_rede_neural_com_embeddings.ipynb)

[Criando uma LSTM para PLN na pr√°tica](LSTM_simples_na_pr√°tica.ipynb)



## An√°lise de Sentimentos

Nestes notebooks, foram utilizadas t√©cnicas diferentes para realizar a an√°lise de sentimentos em uma base de dados com textos de tweets. LSTM (um tipo de rede neural recorrente) e a biblioteca VADER foram utilizadas.

[An√°lise de sentimentos com LSTM](Analise_de_sentimentos_com_LSTM.ipynb)

[An√°lise de sentimentos com VADER](Analise_de_sentimentos_com_VADER.ipynb)

[An√°lise de sentimentos: supervisionado x regras](Analise_de_sentimentos_supervisionado_x_regras.ipynb)



## Fine-tunning em Modelos de Linguagem de Grande Escala

Explora√ß√£o de fine-tunning em LLM realizando ajustes em um modelo pr√©-treinado para uma nova tarefa espec√≠fica, utilizando um conjunto de dados menor e espec√≠fico para a tarefa.

[Implementando fine-Tuning em um LLM usando BERT](implementando_fine_tuning_em_LLM_usando_BERT.ipynb)

[Fine-tunning na pr√°tica com GPT](Fine_tunning_na_pratica_com_GPT.ipynb)

[Implementando LoRA](Implementando_LoRA.ipynb)


## Modelos GPT da OpenAI

Aplica√ß√µes utilizando os modelos dispon√≠veis na OpenAI a partir de uma chave para consulta da API. Com os modelos √© poss√≠vel realizar v√°rias opera√ß√µes relacionadas a Modelos de Linguagem de Grande Escala.

[Testando o modelo GPT da OpenAI](Testando_modelo_GPT_da_OpenAI.ipynb)

[GPT na pr√°tica com GPTNeo](GPT_exemplo_com_GPTNeo.ipynb)

[Constru√ß√£o de agente aut√¥nomo para LLM](Constru√ß√£o_de_agente_aut√¥nomo_para_LLM.ipynb)




## Modelo BERT e Varia√ß√µes

Notebooks utilizando os modelos BERT dispon√≠veis na biblioteca da ü§ó Hugging Face. O modelo √© poderoso, por√©m as aplica√ß√µes nos exemplos possuem um grau de entendimento f√°cil.

[Modelagem de t√≥picos com BERT](Modelagem_de_t√≥picos_com_BERT.ipynb)

[Preenchimento de lacunas com BERTimbau](Preenchimento_de_lacunas_com_BERTimbau.ipynb)

[Preenchimento de lacunas com RoBERTa](Preenchimento_de_lacunas_com_RoBERTa.ipynb)



## Hugging Face e Transformers

Projetos que exploram os m√≥dulos dispon√≠veis na biblioteca da empresa ü§ó Hugging Face. Aplica√ß√µes poderosas no dom√≠nio da Intelig√™ncia Artificial, por√©m com alto n√≠vel de abstra√ß√£o.

[Perguntas e respostas com modelo de LLM da Hugging Face](Perguntas_e_respostas_com_Transformers.ipynb)

[Resumo de textos com modelo de LLM da Hugging Face](Resumo_de_textos_com_Transformers.ipynb)

[Gera√ß√£o de textos com modelo de LLM da Hugging Face](Gera√ß√£o_de_texto_com_Transformers.ipynb)

[Transformers com T5 na pr√°tica para resumo de textos](Transformers_com_T5_na_pratica.ipynb)





---

### üîó Refer√™ncias e Contribui√ß√µes

Boa parte dos c√≥digos s√£o pr√°ticas realizadas em tutoriais e cursos. Segue abaixo lista de links que foram consultados e links de indica√ß√£o de conte√∫do que podem ser utilizados para um estudo mais abrangente.

+ [Forma√ß√£o Processamento de Linguagem Natural e LLM (Udemy)](https://www.udemy.com/course/formacao-processamento-de-linguagem-natural-nlp/?couponCode=THANKSLEARNER24)
+ [LLMs: Dommine GPT, Gemini, BERT e Muito Mais - 2024 (Udemy)](https://www.udemy.com/course/domine-llm/?couponCode=KEEPLEARNING)
+ [PROF. FABIO SANTOS (YouTube)](https://www.youtube.com/@Prof.FabioSantos)

### Licen√ßa

Este projeto n√£o est√° licenciado.

